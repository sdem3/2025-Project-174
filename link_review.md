# Link Review

- 2 articles central to our research
- noteworthy links from those 2 articles
- links to other articles we consider citing in our future article

| Topic | Title | Year | Authors | Paper | Code | Summary |
| :--- | :--- | ---: | :--- | :--- | :--- | :--- |
| Key article 1 | Sign Operator for Coping with Heavy-Tailed Noise | 2025 | Kornilov et al. | [arXiv](https://arxiv.org/abs/2502.07923) | - | Proofs for heavy-tailed noise |
| Key article 2 | Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees | 2021 | Jin, Huang et al. | [arXiv](https://arxiv.org/abs/2002.10940) | - | 2 compressors and proofs of differential privacy |
| Key article 2.1 | Sign-Based Gradient Descent With Heterogeneous Data: Convergence and Byzantine Resilience | 2024 | Jin, Liu, Huang et al. | [IEEE Transactions](https://par.nsf.gov/servlets/purl/10531718) | - | A slightly modified version of key article 2 with a new dataset and no proofs |
| Basic theory | Convex optimization. Cambridge UP. | 2004 | Boyd, S. | [Stanford](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) | - | A go-to for definitions and basic lemmas (see key article 1) |
| Advanced theory | EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback | 2021 | Richtárik et al. | [arXiv](https://arxiv.org/abs/2106.05203) | - | SOTA error feedback algorithm with sound theoretical analysis |
| Advanced theory | A High Probability Analysis of Adaptive SGD with Momentum | 2020 | Li & Orabona | [arXiv](https://arxiv.org/abs/2007.14294) | - | Important Measure Concentration Lemma and an example of proving high-probability convergence |
| Experimental evidence | Why are Adaptive Methods Good for Attention Models? | 2020 | Zhang et al. | [arXiv](https://arxiv.org/abs/1912.03194) | - | 1) Empirical and theoretical evidence that a heavy-tailed distribution of the noise in stochastic gradients causes poor performance of SGD. 2) Gradient clipping can deal with that + BERT training.
| Competitors | From Gradient Clipping to Normalization for Heavy Tailed SGD | 2024 | Hübler et al. | [arXiv](https://arxiv.org/abs/2410.13849) | - | Normalized SGD with heavy tails under heavy-tailed noise and a proof of Batching Lemma |
